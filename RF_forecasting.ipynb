{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb6509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt \n",
    "from numpy import asarray\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ebf522",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cdfb63",
   "metadata": {},
   "source": [
    "# Updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09affbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('all_normalized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd8276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, cols_list ,n_in=1, n_out=1, dropnan=True):\n",
    "    x_cols = cols_list[:-1]\n",
    "    y_cols = cols_list[-1]\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    new_vars = (n_vars - n_out)*n_in\n",
    "    l = x_cols.copy()\n",
    "    for i in range(1, n_in+1):\n",
    "        new_l = []\n",
    "        for j in range(len(x_cols)):\n",
    "            new_l.append(x_cols[j] + \"(t-\" + str(i) + \")\")\n",
    "        l[0:0] = new_l\n",
    "    df = DataFrame(data)\n",
    "    y = df.iloc[:, -1].shift(-1)\n",
    "    df = df.iloc[:, :-1]\n",
    "    cols = list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    # drop rows with NaN values\n",
    "    agg = concat([agg, y], axis = 1)\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg.values, l\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "    return data[:-n_test, :], data[-n_test:, :]\n",
    "\n",
    "# fit an random forest model and make a one step prediction\n",
    "def random_forest_forecast(train, testX, cols, detail):\n",
    "    # transform list into array\n",
    "    train = asarray(train)\n",
    "    # split into input and output columns\n",
    "    trainX, trainy = train[:-1, :-1], train[:-1, -1]\n",
    "    # fit model\n",
    "    n_feature = trainX.shape[0]\n",
    "    feature_list = list(range(n_feature))\n",
    "    model = RandomForestRegressor(n_estimators=1000, n_jobs=-1)\n",
    "    model.fit(trainX, trainy)\n",
    "    importances = list(model.feature_importances_)\n",
    "    imp3 = list()\n",
    "# List of tuples with variable and importance\n",
    "    feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "    feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "    if detail:\n",
    "        for i in range(0, 3):\n",
    "            print((cols[feature_importances[i][0]], feature_importances[i][1]))\n",
    "            imp3.append(cols[feature_importances[i][0]])\n",
    "            \n",
    "    #print(feature_importances[:3])\n",
    "#     [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]\n",
    "    # make a one-step prediction\n",
    "#     print(\"TrainX\")\n",
    "#     print(trainX.shape)\n",
    "#     print(\"TrainY\")\n",
    "#     print(trainy)\n",
    "#     print(\"TestX\")\n",
    "#     print(testX.shape)\n",
    "#     print(testX)\n",
    "    yhat = model.predict([testX])\n",
    "    return yhat[0], imp3\n",
    "\n",
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(data, n_test, cols, detail):\n",
    "    predictions = list()\n",
    "    imp_l = list()\n",
    "    # split dataset\n",
    "    train, test = train_test_split(data, n_test)\n",
    "    # seed history with training dataset\n",
    "    history = [x for x in train]\n",
    "    # step over each time-step in the test set\n",
    "    for i in range(len(test)):\n",
    "        # split test row into input and output columns\n",
    "        testX, testy = test[i, :-1], test[i, -1]\n",
    "        # fit model on history and make a prediction\n",
    "        # Not Anchored\n",
    "        #yhat = random_forest_forecast(history[i:], testX)\n",
    "        # Anchored\n",
    "        yhat, imp3 = random_forest_forecast(history, testX, cols, detail)\n",
    "        # store forecast in list of predictions\n",
    "        predictions.append(yhat)\n",
    "        \n",
    "        for j in range(len(imp3)):\n",
    "            imp_l.append(imp3[j])\n",
    "        \n",
    "        # add actual observation to history for the next loop\n",
    "        history.append(test[i])\n",
    "        # summarize progress\n",
    "        if detail:\n",
    "            print('>expected=%.1f, predicted=%.1f' % (testy, yhat), \"\\n\")\n",
    "    # estimate prediction error\n",
    "#     error = mean_absolute_error(test[:, -1], predictions)\n",
    "    errors = []\n",
    "    for i in range(len(predictions)):\n",
    "        errors.append(abs(test[i, -1] - predictions[i]) / test[i, -1])\n",
    "    error = np.mean(errors)\n",
    "    return error, test[:, -1], predictions, imp_l\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "def data_cleaning(region = \"United States\", date_start = '2020-03-01', date_end = '2021-12-01'):\n",
    "    df_all = pd.read_csv('all_normalized.csv') # read in the original file\n",
    "    df = df_all[['country_name', \n",
    "                 'date', \n",
    "    #                  'stringency_index_x', \n",
    "    #                  'government_response_index', \n",
    "    #                  'containment_health_index', \n",
    "    #                  'economic_support_index', \n",
    "                 'c1_school_closing',\n",
    "                 'c2_workplace_closing',\n",
    "                 'c3_cancel_public_events',\n",
    "                 'c4_restrictions_on_gatherings',\n",
    "                 'c5_close_public_transport',\n",
    "                 'c6_stay_at_home_requirements',\n",
    "                 'c7_movementrestrictions',\n",
    "                 'c8_internationaltravel',\n",
    "                 'e1_income_support',\n",
    "                 'e2_debtrelief',\n",
    "                 'h1_public_information_campaigns',\n",
    "                 'h2_testing_policy',\n",
    "                 'h3_contact_tracing',\n",
    "                 'h6_facial_coverings',\n",
    "                 'h7_vaccination_policy',\n",
    "                 'h8_protection_of_elderly_people',\n",
    "                 'new_cases_smoothed_per_million']]\n",
    "\n",
    "    df = df.loc[df['country_name'] == region]\n",
    "    df.set_index(\"date\",inplace=True)\n",
    "    df.loc[date_start:date_end]\n",
    "    df = df.dropna(axis=0)\n",
    "    df = df.loc[date_start:date_end]\n",
    "    df[\"date_count\"] = list(range(df.shape[0]))\n",
    "\n",
    "\n",
    "    # dummies=pd.concat((pd.get_dummies(df['country_name']), df),axis=1) # create dummies on country names\n",
    "    # dummies.set_index(\"date\",inplace=True) # set date as index\n",
    "    # dummies.index=pd.to_datetime(dummies.index)\n",
    "    # ds=dummies.index.to_series()\n",
    "    # dummies['Month']=ds.dt.month # create dummies for date\n",
    "    # dummies['Day']=ds.dt.day\n",
    "    # dummies['Year']=ds.dt.year\n",
    "\n",
    "\n",
    "\n",
    "    # dummies = dummies.loc[date_start:date_end]\n",
    "    # dummies = dummies.drop('country_name', axis = 1)\n",
    "    # df = dummies.dropna(axis=0)\n",
    "    # df\n",
    "    cols = df.columns.tolist()\n",
    "    y = 'new_cases_smoothed_per_million'\n",
    "    cols = cols[1:-2]\n",
    "    cols.append('date_count')\n",
    "    cols.append(y)\n",
    "    df = df[cols]\n",
    "\n",
    "    cols_list = df.columns.tolist()\n",
    "    df_array = df.to_numpy()\n",
    "    return df, df_array, cols_list\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "# df = pd.DataFrame({'x1':[1,2,3,4,5, 6, 7, 8], \n",
    "#                    'x2':[10,20,40,50,60, 70, 80 , 90], \n",
    "#                    'y1': [1,4,9,16,25, 36, 49, 64]})\n",
    "# print(df)\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "# n_row = df.shape[0]\n",
    "# cols_list = df.columns.tolist()\n",
    "# df_array = df.to_numpy()\n",
    "\n",
    "# df, df_array, cols_list = data_cleaning(region = \"United States\", date_start = '2020-03-01', date_end = '2021-12-01')\n",
    "\n",
    "# data, col_names = series_to_supervised(df_array, cols_list, n_in=30 , n_out=1)\n",
    "# # evaluate\n",
    "# mre, y, yhat = walk_forward_validation(data, n_test = 30, cols = col_names)\n",
    "# print('MRE: %.3f' % mre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c6f8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_model(region = \"United States\", \n",
    "             date_start = '2020-03-01', \n",
    "             date_end = '2021-12-01',\n",
    "             n_in = 30, \n",
    "             n_out = 1, \n",
    "             n_test = 30,\n",
    "             detail = True):\n",
    "    df, df_array, cols_list = data_cleaning(region, date_start, date_end)\n",
    "    data, col_names = series_to_supervised(df_array, cols_list, n_in , n_out)\n",
    "    # evaluate\n",
    "    mre, y, yhat, importance_variables = walk_forward_validation(data, n_test, col_names, detail)\n",
    "    print('MRE: %.3f' % mre)\n",
    "    plt.plot(y, label='Actual')\n",
    "    plt.plot(yhat, 'r-', markersize=3, label='Prediction' )\n",
    "    plt.ylabel('new_cases_smoothed_per_million')\n",
    "    plt.xlabel(\"Test days\")\n",
    "    plt.title(\"RF in \" + region)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    frequencies = dict(zip(col_names, [0] * len(col_names)))\n",
    "    for var in importance_variables:\n",
    "        frequencies[var] += 1\n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498eeb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = ['Taiwan', \n",
    "           'Croatia', \n",
    "           'Afghanistan', \n",
    "           'Iran', \n",
    "           'New Zealand', \n",
    "           'Macao', \n",
    "           'Estonia', \n",
    "           'China',\n",
    "          'Belarus', \n",
    "           'Cambodia',\n",
    "           'Russia', \n",
    "           'Senegal', \n",
    "           'Lithuania', \n",
    "           'Nigeria'\n",
    "          ]\n",
    "\n",
    "\n",
    "ith = 0\n",
    "\n",
    "for region in regions:\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(region)\n",
    "    dic1 = rf_model(region = region, \n",
    "             date_start = '2020-03-01', \n",
    "             date_end = '2021-12-01',\n",
    "             n_in = 30, \n",
    "             n_out = 1, \n",
    "             n_test = 30,\n",
    "        detail = True)\n",
    "    \n",
    "    if ith == 0:\n",
    "        df_dic = pd.DataFrame.from_dict(dic1,  orient='index').T\n",
    "        df_dic.index = [region]\n",
    "    else:\n",
    "        df_dic_temp = pd.DataFrame.from_dict(dic1,  orient='index').T\n",
    "        df_dic_temp.index = [region]\n",
    "        print(df_dic_temp)\n",
    "        df_dic = df_dic.append(df_dic_temp)\n",
    "    ith += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_all[['country_name', 'continent']].groupby(['country_name', 'continent']).size().reset_index().rename(columns={0:'count'})\n",
    "l1 = df_temp[['country_name']].values.reshape(1, -1).tolist()[0]\n",
    "l2 = df_temp[['continent']].values.reshape(1, -1).tolist()[0]\n",
    "dic_cc = dict(zip(l1, l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be40cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df_dic.T[(df_dic.sum(axis = 0) != 0)].rename(dic_cc, axis='columns').T.groupby(level=0).sum().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a3928",
   "metadata": {},
   "source": [
    "Only extracting top 10 importance Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784ffe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = final_df.sum(axis =  1).sort_values(ascending = False)\n",
    "l_name = a[:10].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c16ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.T[l_name].T.plot.bar(sort_columns = True, stacked = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c25cba",
   "metadata": {},
   "source": [
    "If you want to see the all stacked bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067eb063",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dic.T[(df_dic.sum(axis = 0) != 0)].rename(dic_cc, axis='columns').T.groupby(level=0).sum().T.plot.bar(stacked = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
